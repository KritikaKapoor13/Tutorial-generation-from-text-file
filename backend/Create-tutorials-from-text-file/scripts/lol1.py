elements=['''<h2>Federated Learning: Challenges, Methods, and Future Directions''', '''<p>Tian Li Anit Kumar Sahu Carnegie Mellon University Bosch Center for Artificial Intelligence tianli@cmu.edu anit.sahu@gmail.com Ameet Talwalkar Virginia Smith Carnegie Mellon University & Determined AI Carnegie Mellon University talwalkar@cmu.edu smithv@cmu.edu Abstract''', '''<s13>Federated learning involves training statistical models over remote devices or siloed data centers, such''', '''<h49>as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially''', '''<h47>massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data''', '''<s19>analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide''', '''<s8>a broad overview of current approaches, and outline several directions of future work that are relevant to''', '''<p>a wide range of research communities.''', '''<h3>Introduction''', '''<h7>Mobile phones, wearable devices, and autonomous vehicles are just a few of the 
modern distributed''', '''<h44>networks generating a wealth of data each day. Due to the growing computational power of these devices''', '''<h7>coupled with concerns over transmitting private informationit is increasingly attractive to store data''', '''<h25>locally  and push network computation to the edge.''', '''<h7>The concept of edge computing is not a new one. Indeed, computing simple queries across distributed,''', '''<h43>low-powered devices is a decades-long area of research that has been explored under the purview of query''', '''<h44>processing in sensor networks, computing at the edge, and fog computing [''', '''<h25>12  29  40  49  74''', '''<h44>]. Recent works''', '''<h28>have also considered training machine learning models centrally but serving and storing them locally; for''', '''<h25>example, this is a common approach in mobile user 
modeling and personalization [60, 90].''', '''<h44>However, as the storage and computational capabilities of the devices within distributed networks grow, it is''', '''<h38>possible to leverage enhanced local resources on each device. This has led to a growing interest in  federated''', '''<h19>learning  [''', '''<h25>75''', '''<h19>], which explores  training  statistical models directly on remote devices . As we discuss in this''', '''<h37>article, learning in such a setting differs significantly from traditional distributed environmentsrequiring''', '''<s20>We use the term device throughout the article to describe entities in the network, such as nodes, clients, sensors, or organizations.''', '''<h1>arXiv:1908.07873v1  [cs.LG]  21 Aug 2019''', '''<h46>local  updates local  updates new global model''', '''<s23>local data local data local data local data local data local data local data local data''', '''<s28>learnt model: next-word prediction''', '''<h7>Figure 1: An example application of federated learning for the task 
of next-word prediction on mobile phones. To preserve the privacy of the text data and to reduce strain on the network, we seek to train a''', '''<h44>predictor in a distributed fashion, rather than sending the raw data to a central server. In this setup, remote''', '''<h14>devices communicate with a central server periodically to learn a global model. At each communication''', '''<h15>round, a subset of selected phones performs local training on their non-identically-distributed user data,''', '''<h44>and sends these local updates to the server. After incorporating the updates, the server then sends back the''', '''<h42>new global model to another subset of devices. This iterative training process continues across the network''', '''<h25>until convergence is reached or some stopping criterion is met.''', '''<h39>fundamental advances in areas such as privacy, large-scale machine learning, and distributed optimization,''', '''<h26>and raising new questions at the intersection of diverse fields, such as machine learning and systems [''', '''<h25>91''', '''<h26>].''', '''<h11>Federated learning methods have been deployed by major service providers [''', '''<h25>11  124''', '''<h11>], and play a critical''', '''<h19>role in supporting privacy-sensitive applications where the training data are distributed at the edge [e.g.,''', '''<h25> 46  51  89  105  127  139''', '''<h7>]. Examples of potential applications include: learning sentiment, semantic location, or activities of mobile phone users; adapting to pedestrian behavior in 
autonomous vehicles; and predicting health events like heart attack risk from wearable devices [''', '''<h25> 52  84''', '''<h7>]. We discuss several''', '''<h25>canonical applications of federated learning below:''', '''<h44> Smart phones.  By jointly learning user behavior across a large pool of mobile phones, statistical models''', '''<h13>can power applications such as next-word prediction, face detection, and voice recognition [''', '''<h25>46  89''', '''<h13>].''', '''<h7>However, users may not be willing to share their data in order to protect their personal privacy or to save the limited bandwidth/battery power of their phone. Federated learning has the potential to enable predictive features on smart phones without diminishing the user experience or leaking private information. Figure 1 depicts one such application in which we aim to learn a next-word''', '''<h25>predictor in a large-scale mobile phone network based on users historical text data [46].''', '''<h44> Organizations.  Organizations or institutions can also be viewed as devices in the context of federated''', '''<h7>learning. For example, hospitals are organizations that contain a multitude of patient data for''', '''<h20>predictive healthcare. However, hospitals operate under strict privacy practices, and may face legal,''', '''<h7>administrative, or ethical constraints that require data to remain local. Federated learning is a promising solution for these applications [''', '''<h25>52''', '''<h7>], as it can reduce strain on the network and enable''', '''<h25>private learning between various devices/organizations.''', '''<h7> Internet of things.  Modern IoT networks, such as wearable devices, autonomous vehicles, or smart''', '''<h25>homes, may contain numerous sensors that allow them to collect, react, and adapt to incoming data''', '''<img> p1-0.png''', '''<img> p1-1.png''', '''<img> p1-2.png''', '''<img> p1-3.png''', '''<img> p1-4.png''', '''<img> p1-5.png''', '''<img> p1-6.png''', '''<img> p1-7.png''', '''<img> p1-8.png''', '''<img> p1-9.png''', '''<img> p1-10.png''', '''<h28>in real-time. For example, a fleet of autonomous vehicles may require an up-to-date model of traffic,''', '''<h27>construction, or pedestrian behavior to safely operate. However, building aggregate models in these''', '''<h18>scenarios may be difficult due to the private nature of the data and the limited connectivity of each''', '''<h44>device. Federated learning methods can help to train models that efficiently adapt to changes in these''', '''<h25>systems while maintaining user privacy [84, 98].''', '''<h5>1.1 Problem 
Formulation''', '''<h44>The canonical federated learning problem involves learning a  single, global  statistical model from data stored''', '''<h7>on tens to potentially millions of remote devices. We aim to learn this model under the constraint that''', '''<h44>device-generated data is stored and processed locally, with only intermediate 
updates being communicated''', '''<h7>periodically with a central server. In particular, the goal is typically to minimize the following objective''', '''<h25>function: min 
 , where  F  :  . (1)''', '''<h7>Here,''', '''<h25> m''', '''<h7> is the total number of devices,''', '''<h25> p''', '''<h6> ''', '''<h7> 0 and''', '''<h25>   p''', '''<h6> =''', '''<h7> 1, and''', '''<h25> F''', '''<h7> is the local objective function for the''', '''<h25> k''', '''<h7>th device. The local objective function is often defined as the empirical risk over local data, i.e.,''', '''<h6>) =''', '''<h25> x  y''', '''<h41>, where''', '''<h25> n''', '''<h41> is the number of samples available locally. The user-defined term''', '''<h7> specifies the relative 
impact of each device, with two natural settings being''', '''<h25> p''', '''<h6> =''', '''<h7>or''', '''<h25> p''', '''<h6> =''', '''<s22> n''', '''<h7>, where''', '''<h6> =''', '''<h25>   n''', '''<h7> is the total number of samples. We will reference problem''', '''<h25> (1)''', '''<h7> throughout the article, but, as''', '''<h28>discussed below, we note that other objectives 
or modeling approaches may be appropriate depending on''', '''<h25>the application of interest.''', '''<h5>1.2 Core Challenges''', '''<h30>We next describe four of the core challenges associated with solving the distributed optimization problem''', '''<h11>posed in''', '''<h25> (1)''', '''<h11>. These challenges make the federated setting distinct from other classical problems, such as''', '''<h25>distributed learning in data center settings or traditional private data analyses. Challenge 1: Expensive Communication.''', '''<h7> Communication is a critical bottleneck in federated networks, which, coupled with privacy concerns over sending raw data, necessitates that data generated on 
each''', '''<h39>device remain local. Indeed, federated networks are potentially comprised of a massive number of devices,''', '''<h40>e.g., millions of smart phones, and communication in the network can be slower than local computation by''', '''<h44>many orders of magnitude [''', '''<h25>50  115''', '''<h44>]. In order to fit a model to data generated by the devices in the federated''', '''<h21>network, it is therefore necessary to develop communication-efficient methods that iteratively send small''', '''<h35>messages or  model updates  as part of the training process, as opposed to sending the entire dataset over the''', '''<h28>network. To further reduce communication in such a 
setting, two key aspects to consider are: (i) reducing''', '''<h43>the total number of communication rounds, or (ii) reducing the size of transmitted messages at each round.''', '''<h25>Challenge 2: Systems Heterogeneity.''', '''<h7> The storage, computational, and communication capabilities of each device in federated networks may differ due to variability in hardware (CPU, memory), network''', '''<h44>connectivity (3G, 4G, 5G, wifi), and power (battery level). Additionally, the network size and systems-related''', '''<h24>constraints on each device typically result in only a small fraction of the devices being active at once, e.g.,''', '''<h21>hundreds of active devices in 
a million-device network [''', '''<h25>11''', '''<h21>]. Each device may also be unreliable, and it is''', '''<h44>not uncommon for an active device to drop out at a given iteration due to connectivity or energy constraints.''', '''<h38>These system-level characteristics dramatically exacerbate challenges such as straggler mitigation and fault''', '''<h43>tolerance. Federated learning methods that are developed and analyzed must therefore: (i) anticipate a low''', '''<h37>amount of participation, (ii) tolerate heterogeneous hardware, and (iii) be robust to dropped devices in the''', '''<h25>network. Challenge 3: Statistical Heterogeneity.''', '''<h10> Devices frequently generate and 
collect data in a non-identically''', '''<h42>distributed manner across the network, e.g., mobile phone users have varied use of language in the context''', '''<h31>of a next word prediction task. Moreover, the number of data points across devices may vary significantly,''', '''<h7>and there may be an underlying structure present that captures the relationship amongst devices and''', '''<h9>their associated distributions. This data generation paradigm violates frequently-used independent and''', '''<h44>identically distributed (I.I.D.) assumptions in distributed optimization, increases the likelihood of stragglers,''', '''<h15>and may add complexity in terms of modeling, analysis, and evaluation. Indeed, although the canonical''', '''<h37>federated learning problem of''', '''<h25> (1)''', '''<h37> aims to learn a single global model, there exist other alternatives such as''', '''<h27>simultaneously learning distinct local models via multi-task learning frameworks [cf.''', '''<h25> 106''', '''<h27>]. There is also a''', '''<h28>close connection in this regard between leading approaches for federated learning and meta-learning [''', '''<h25>64''', '''<h28>].''', '''<h33>Both the multi-task and meta-learning perspectives enable  personalized  or  device-specific  modeling, which is''', '''<h25>often a more natural approach to handle the statistical heterogeneity of the data. Challenge 4: Privacy Concerns.''', '''<h44> Finally, privacy is often a major concern in federated learning applications.''', '''<h7>Federated learning makes a step towards protecting data generated on each device by sharing model''', '''<h14>updates, e.g., gradient information, instead of the raw data [''', '''<h25>17  31  33''', '''<h14>]. However, communicating model''', '''<h44>updates throughout the training process can nonetheless reveal sensitive information, either to a third-party,''', '''<h7>or to the central server [''', '''<h25>76''', '''<h7>]. While recent methods aim to enhance the privacy of federated learning''', '''<h24>using tools such as secure multiparty computation or differential privacy, these approaches often provide''', '''<h41>privacy at the cost of reduced model performance or system efficiency. Understanding and balancing 
these''', '''<h7>trade-offs, both theoretically and empirically, is a considerable challenge in realizing private federated''', '''<h25>learning systems.''', '''<h44>The remainder of this article is organized as follows. In Section 2, we introduce previous and current works''', '''<h7>that aim to address the four discussed challenges of federated learning. In Section 3, we outline several''', '''<h25>promising directions of future research.''', '''<h3>Survey of Related and Current Work''', '''<h7>The challenges in federated learning at first glance resemble classical problems in areas such as privacy, large-scale machine learning, and distributed optimization. For instance, numerous methods have been''', '''<h34>proposed to tackle expensive communication in the machine learning, optimization, and signal processing''', '''<h37>communities. However, these methods are typically unable to fully handle the scale of federated networks,''', '''<h24>much less the challenges of systems and statistical heterogeneity. Similarly, while privacy is an important''', '''<h30>aspect for many machine learning applications, privacy-preserving methods for federated learning can be''', '''<h28>challenging to rigorously assert due to the statistical variation in the data, and may be even more difficult''', '''<h44>to implement due to systems constraints on each device and across the potentially massive network. In this''', '''<h34>section, we explore in more detail the challenges presented in Section 1, including a discussion of classical''', '''<h25>results as well as more recent work focused specifically on federated learning.''', '''<s30>mini-batch data computation communication''', '''<s31>apply updates locally''', '''<h14>Figure 2:  Left: Distributed (mini-batch) SGD.  Each device,''', '''<h25> k''', '''<h14>, locally computes gradients from a mini-batch''', '''<h13>of data points to approximate''', '''<h6> ''', '''<h13>, and the aggregated mini-batch updates are applied on the server.''', '''<h7>Right: Local updating schemes.  Each device immediately applies local updates, e.g., gradients, after they are computed and a server performs a global aggregation after a variable number of local updates.''', '''<h25>Local-updating schemes can reduce communication by performing additional work locally.''', '''<h5>2.1 Communication-efficiency''', '''<h41>Communication is a key bottleneck to consider when developing methods for federated networks. While it''', '''<h43>is beyond the scope of this article to provide a self-contained review of communication-efficient distributed''', '''<h44>learning methods, we point out several general directions, which we group into (1) local updating methods,''', '''<h25>(2) compression schemes, and (3) 
decentralized training. 2.1.1 Local Updating''', '''<h44>Mini-batch optimization methods, which involve extending classical stochastic methods to process multiple''', '''<h38>data points at a time, have emerged as a popular paradigm for distributed machine learning in data center''', '''<h44>environments [''', '''<h25>28  88  96  102  103''', '''<h44>]. In practice, however, they have been shown to have limited flexibility to''', '''<h22>adapt to communication-computation trade-offs that would maximally leverage distributed data process-''', '''<h44>ing [''', '''<h25>107  108''', '''<h44>]. In response, several recent methods have been proposed to improve communication-efficiency''', '''<h7>in distributed settings by allowing for a variable number of  local updates  to be applied on each machine in parallel at each communication round, making the amount of computation versus communication substantially more flexible. For convex objectives, distributed local-updating  primal-dual  methods have emerged as a popular way to tackle such a problem [''', '''<h25>54  62  72  107  128''', '''<h7>]. These approaches leverage duality structure to effectively decompose the global objective into subproblems that can be solved in''', '''<h18>parallel at each communication round. Several distributed local-updating  primal  methods have also been''', '''<h7>proposed, which have the added benefit of being applicable to non-convex objectives [''', '''<h25>93  136''', '''<h7>]. These''', '''<h44>methods drastically improve performance in practice, and have been shown to achieve orders-of-magnitude''', '''<h7>speedups over traditional mini-batch methods or distributed approaches like ADMM [''', '''<h25>14''', '''<h7>] in real-world''', '''<h25>data center environments. We provide an intuitive illustration of local updating methods in Figure 2.''', '''<h44>In federated settings, optimization methods that allow for flexible local updating and low client participation''', '''<h7>have become the de facto solvers [''', '''<h25>65  75  106''', '''<h7>]. The most commonly used method for federated learning is Federated Averaging (''', '''<h25>FedAvg''', '''<h7>) [''', '''<h25>75''', '''<h7>], a method based on averaging local stochastic gradient descent''', '''<h16>(SGD) updates for the primal problem.''', '''<h25> FedAvg''', '''<h16> has been shown to work well empirically, particularly for''', '''<h7>non-convex problems, but comes without convergence guarantees and can diverge in practical settings when data are heterogeneous [''', '''<h25>65''', '''<h7>]. We discuss methods to handle such statistical heterogeneity in more''', '''<img> p4-0.png''', '''<img> p4-1.png''', '''<img> p4-2.png''', '''<img> p4-3.png''', '''<img> p4-4.png''', '''<img> p4-5.png''', '''<img> p4-6.png''', '''<img> p4-7.png''', '''<h25>detail in Section 2.3.2.''', '''<h44>Figure 3: Centralized vs. decentralized topologies. In the typical federated learning setting and as a focus of''', 
'''<h37>this article, we assume a star network (left) where a server connects with all remote devices. Decentralized''', '''<h25>topologies (right) are a potential alternative when communication to the server becomes a bottleneck. 2.1.2 Compression Schemes''', '''<h7>While local updating methods can reduce the total  number of communication rounds , model compression''', '''<h29>schemes such as sparsification, subsampling, and quantization can significantly reduce the  size of messages''', '''<h7>communicated at each round. These methods have been extensively studied, both empirically and theoretically, in previous literature for distributed training in data center environments; we defer the readers to [''', '''<h25>119  135''', '''<h7>] for a more complete review. In federated environments, the low participation of devices, non-identically distributed local data, and local updating schemes pose novel challenges to these model compression approaches. For instance, the commonly-used error compensation 
techniques in classical distributed learning [''', '''<h25>101''', '''<h7>] cannot be directly extended to federated settings as the errors''', '''<h8>accumulated locally may be stale if the devices are not frequently sampled. Nevertheless, several works''', '''<h21>have provided practical strategies in federated settings, such as forcing the updating models to be sparse''', '''<h7>and low-rank; performing quantization with structured random rotations [''', '''<h25>59''', '''<h7>]; using lossy compression''', '''<h26>and 
dropout to reduce server-to-device communication [''', '''<h25>15''', '''<h26>]; and applying Golomb lossless encoding [''', '''<h25>99''', '''<h26>].''', '''<h28>From a theoretical perspective, while prior work has explored convergence guarantees with low-precision''', '''<h17>training in the presence of non-identically distributed data [e.g.,''', '''<h25> 111''', '''<h17>], the assumptions made do not take''', '''<h7>into consideration common characteristics of the federated setting, such as low device participation or''', '''<h25>locally-updating optimization methods. 2.1.3 Decentralized Training''', '''<h7>In federated learning, a star network (where a central server is connected to a network of devices, as in the left panel of Figure 3) is the predominant communication topology; we therefore focus on the''', '''<h8>star-network setting in this article. However, we briefly discuss decentralized topologies (where devices''', '''<h32>only communicate with their neighbors, e.g., the right panel of Figure 3) as a potential alternative. In data''', '''<h18>center environments, decentralized training has been demonstrated to be faster than centralized training''', '''<h22>when operating on networks with low bandwidth or high latency; we defer readers to [''', '''<h25>47  67''', '''<h22>] for a more''', '''<h28>comprehensive review. Similarly, in federated learning, decentralized algorithms can in theory reduce the''', '''<h35>high communication cost on the central server. Some recent works [''', '''<h25>47  61''', '''<h35>] have investigated decentralized''', '''<h41>training over heterogeneous data with local updating schemes. However, they are either restricted to linear''', '''<h43>models [''', '''<h25>47''', '''<h43>] 
or assume full device participation [''', '''<h25>61''', '''<h43>]. Finally, hierarchical communication patterns have also''', '''<h9>been proposed [''', '''<h25>68  70''', '''<h9>] to 
further ease the burden on the central server, by first leveraging  edge servers  to''', '''<h44>aggregate the updates from edge devices and then relying on a  cloud server  to aggregate updates from edge''', '''<h23>servers. While this is a promising approach to reduce communication, it is not applicable to all networks,''', '''<img> p5-0.png''', '''<img> p5-1.png''', '''<img> p5-2.png''', '''<img> p5-3.png''', '''<img> p5-4.png''', '''<img> p5-5.png''', '''<img> p5-6.png''', '''<img> p5-7.png''', '''<img> p5-8.png''', '''<img> p5-9.png''', '''<img> p5-10.png''', '''<h25>as this type of physical hierarchy may not exist or be known a priori.''', '''<h45>4G training training subsample devices subsample devices training training send the global model send the  global model send model updates device failure''', '''<h22>Figure 4: Systems heterogeneity in federated learning. Devices may vary in terms of network connection,''', '''<h7>power, and hardware. Moreover, some of the devices may drop at any time during training. Therefore,''', '''<h13>federated training methods must tolerate heterogeneous systems environments and low participation of''', '''<h25>devices, i.e., they must allow for only a small subset of devices to be active at each round.''', '''<h5>2.2 Systems Heterogeneity''', '''<h7>In federated settings, there is significant variability in the  systems  characteristics across the network, as''', '''<h38>devices may differ in terms of hardware, network connectivity, and battery power. As depicted in Figure 4,''', '''<h7>these systems characteristics make issues such as stragglers significantly more prevalent than in typical''', '''<h23>data center environments. We roughly group several key directions to handle 
systems heterogeneity into:''', '''<h7>(i) asynchronous communication, (ii) active device sampling, and (ii) fault tolerance. As mentioned in''', '''<h25>Section 2.1.3, we assume a star topology in our following discussions. 2.2.1 Asynchronous Communication''', '''<h7>In traditional data center settings, synchronous and asynchronous schemes are both commonly used to parallelize iterative optimization algorithms, with each approach having pros and cons. Synchronous schemes are simple and guarantee a serial-equivalent computational model, but they are also more''', '''<h38>susceptible to stragglers in the face of device variability. Asynchronous schemes are an attractive approach''', '''<h44>to mitigate stragglers in heterogeneous environments, particularly in shared-memory systems [''', '''<h25>27  30  48  92 141''', '''<h36>]. However, they 
typically rely on bounded-delay assumptions to control the degree of staleness, which''', '''<h7>for device''', '''<h25> k''', '''<h7> depends on the number of other devices that have updated since device''', '''<h25> k''', '''<h7> pulled from the''', '''<h41>central server. While asynchronous parameter servers have been successful in distributed data 
centers [e.g.,''', '''<h25>27  48  141''', '''<h26>], classical bounded-delay assumptions can be unrealistic in federated settings, where the delay''', '''<h25>may be on the order of hours to days, or completely unbounded. 2.2.2 Active Sampling''', '''<h7>In federated networks, typically only a small subset of devices participate at each round 
of training. However, the vast majority of federated methods, e.g. those described in [''', '''<h25>11  47  65  75  106''', '''<h7>], are  passive''', '''<h9>in that they do not aim to influence which devices participate. An alternative approach involves  actively''', '''<h27>selecting participating devices at each round. For example, Nishio and Yonetani''', '''<h25> [83]''', '''<h27> explore novel device''', '''<h7>sampling policies based on systems resources, with the aim being for the server to aggregate as many''', 
'''<h29>device updates as possible within a pre-defined time window. Similarly, Kang et al.''', '''<h25> [57]''', '''<img> p6-0.png''', '''<img> p6-1.png''', '''<img> p6-2.png''', '''<img> p6-3.png''', '''<img> p6-4.png''', '''<img> p6-5.png''', '''<img> p6-6.png''', '''<img> p6-7.png''', '''<img> p6-8.png''', '''<img> p6-9.png''', '''<img> p6-10.png''', '''<h29> take into account''', '''<h13>systems overheads incurred on each device when designing incentive mechanisms to encourage devices''', '''<h7>with higher-quality data to participate in the learning process. However, these methods assume a static model of the systems characteristics of the network; it remains open how to extend these approaches to''', '''<h21>handle  real-time , device-specific fluctuations in computation and communication delays. Moreover, while''', '''<h7>these methods primarily focus on systems variability 
to perform active sampling, we note that it is also worth considering actively sampling a set of small but sufficiently representative devices based on the''', '''<h25>underlying  statistical  structure. 2.2.3 Fault Tolerance''', '''<h43>Fault tolerance has been extensively studied in the systems community and is a fundamental consideration''', '''<h44>of classical distributed systems [''', '''<h25>19  71  110''', '''<h44>]. Recent works have also investigated fault tolerance specifically''', '''<h7>for machine learning workloads in data center environments [e.g.,''', '''<h25> 87  112''', '''<h7>]. When learning over remote''', '''<h17>devices, however, fault tolerance becomes more critical as it is common for some participating devices to''', '''<h28>drop out at some point before the completion of the given training iteration [''', '''<h25>11''', '''<h28>]. One practical strategy is''', '''<h37>to simply ignore such device failure [''', '''<h25>11''', '''<h37>], which may introduce bias into the device sampling scheme if the''', '''<h44>failed devices have specific data characteristics. For instance, devices from remote areas may be more likely''', '''<h7>to drop due to poor network connections and thus the trained federated model will be biased towards devices with favorable network conditions. Theoretically, while several recent works have investigated''', '''<h12>convergence guarantees of variants of federated learning methods [''', '''<h25>56  123  131  132''', '''<h12>], few analyses allow''', '''<h25>for low participation [e.g., 65, 106], or study directly the effect of dropped devices.''', '''<h44>Coded computation  is another option to tolerate device failures by introducing algorithmic redundancy. Recent works have explored using codes to speed up distributed machine learning training [e.g.,''', '''<h25> 20  21  63  94  109''', '''<h44>].''', '''<h27>For instance, in the presence of stragglers, gradient coding and its variants [''', '''<h25>20  21  109''', '''<h27>] carefully replicate''', '''<h18>data blocks (as well as the gradient computation 
on those data blocks) across computing nodes to obtain''', '''<h7>either exact or inexact recovery of the true gradients. While this is a seemingly promising approach for the federated setting, these methods face fundamental challenges in federated networks as sharing''', '''<h25>data/replication across devices is often infeasible due to privacy constraints and the scale of the network.''', '''<h5>2.3 Statistical Heterogeneity''', '''<h44>Challenges arise when training federated models from data that is not identically distributed across devices,''', '''<h9>both in terms of modeling the data (as depicted in Figure 5), and in terms of analyzing the convergence''', '''<h25>behavior of associated training procedures. We discuss related work in these directions below. 2.3.1 Modeling Heterogeneous Data''', '''<h7>There exists a large body of literature in machine learning that has modeled statistical heterogeneity''', '''<h8>via methods such as meta-learning [''', '''<h25>114''', '''<h8>] and multi-task learning [''', '''<h25>18  37''', '''<h8>]; these ideas have been recently''', '''<h7>extended to the federated setting [''', '''<h25>24  26  35  58  106  138''', '''<h7>]. For instance,''', '''<h25> MOCHA''', '''<h7> [''', '''<h25>106''', '''<h7>], an optimization''', '''<h42>framework designed for the federated setting, can allow for personalization by learning  separate  but related''', '''<h30>models for each device while leveraging a shared representation via multi-task learning. This method has''', '''<h7>provable theoretical convergence guarantees for the considered objectives, but is limited in its ability to''', '''<h47>(a) Learn personalized''', '''<s19>models for each device; do''', '''<p>not learn from peers.''', '''<h47>(b) Learn a global model;''', '''<p>learn from peers.''', '''<h47>(c) Learn personalized models for each device;''', '''<p>learn from peers.''', '''<h7>Figure 5: Different modeling approaches in federated networks. Depending on properties of the data,''', '''<h27>network, and application of interest, one may choose to (a) learn separate models for each device, (b) fit a''', '''<h25>single global model to all devices, or (c) learn related but distinct models in the network.''', '''<h7>scale to massive networks and is restricted to convex objectives. Another approach [''', '''<h25>26''', '''<h7>] models the star''', '''<h40>topology as a Bayesian network and performs variational inference during learning. Although this method''', '''<h42>can handle non-convex models, it is expensive to generalize to large federated networks. Khodak et al.''', '''<h25> [58]''', '''<h42>provably meta-learn a within-task learning rate using multi-task information (where each task corresponds''', '''<h7>to a device) and have demonstrated improved empirical performance over vanilla''', '''<h25> FedAvg''', '''<h7>. Eichner et al.''', '''<h25>[35]''', '''<h7> investigate a pluralistic solution (adaptively choosing between a global model and device-specific''', '''<h29>models) to address the cyclic patterns in data samples during federated training. Zhao et al.''', '''<h25> [138]''', '''<h29> explore''', '''<h7>transfer learning for personalization by running''', '''<h25> FedAvg''', '''<h7> after training a global model centrally on some shared proxy data. Despite these recent advances, key challenges still remain in making methods for''', '''<h25>heterogeneous modeling that are robust, scalable, and automated in federated settings.''', '''<h7>When modeling federated data, it may also be important to consider issues beyond accuracy, such as''', '''<h10>fairness . In particular, naively solving an aggregate loss function such as in''', '''<h25> (1)''', '''<h10> may implicitly advantage''', '''<h7>or disadvantage some of the devices, as the learned model may become biased towards devices with larger amounts of data, or (if weighting devices equally), to commonly occurring groups of devices.''', '''<h28>Recent works have proposed modified modeling approaches that aim to reduce the variance of the model''', '''<h7>performance across devices. Some heuristics simply perform a varied number of local updates based on local loss [''', '''<h25>52''', '''<h7>]. Other more principled approaches include Agnostic Federated Learning [''', '''<h25>80''', '''<h7>], which''', '''<h30>optimizes the centralized model for any target distribution formed by a mixture of the client distributions''', '''<h7>via a minimax optimization scheme. Another more general approach is taken by Li et al.''', '''<h25> [66]''', '''<h7>, which proposes an objective called''', '''<h25> q''', '''<h7>-FFL in which devices with higher loss are given higher relative weight to''', '''<h19>encourage less variance in the final accuracy distribution. Beyond issues of fairness, we note that aspects''', '''<h35>such as accountability and interpretability in federated learning are additionally worth exploring, but may''', '''<h25>be challenging due to the scale and heterogeneity of the network. 2.3.2 Convergence Guarantees for Non-IID Data''', '''<h7>Statistical heterogeneity also presents novel challenges in terms of analyzing the convergence behavior in federated settingseven when learning a single global model. Indeed, when data is not identically distributed across devices in the network, methods such as''', '''<h25> FedAvg''', '''<h7> have been shown to diverge in''', '''<h40>practice [''', '''<h25>65  75''', '''<h40>]. Parallel SGD and related variants, which make local updates similar to''', '''<h25> FedAvg''', '''<h40>, have been''', '''<h7>analyzed in the I.I.D. setting [''', '''<h25>68  93  104  108  120  121  122  125  136  140''', '''<h7>]. However, the results rely on''', '''<h9>the premise that each local solver is a copy of the same stochastic process (due to the I.I.D. assumption),''', '''<h41>which is not the case in typical federated settings. To understand the performance of''', '''<h25> FedAvg''', '''<h41> in statistically''', '''<h7>heterogeneous settings,''', '''<h25> FedProx''', '''<h7> [''', '''<h25>65''', '''<h7>] has recently been proposed.''', '''<h25> FedProx''', '''<h7> makes a small modification to the''', '''<h25> FedAvg''', '''<h7> method to help ensure convergence, both theoretically and in practice.''', '''<h25> FedProx''', '''<h7> can also''', '''<h16>be interpreted as a generalized, reparameterized version of''', '''<h25> FedAvg''', '''<h16> that has practical ramifications in the''', '''<h44>context of accounting for systems heterogeneity across devices. Several other works [''', '''<h25>56  123  131  132''', '''<h44>] have also explored convergence guarantees in the presence of heterogeneous data with different assumptions, e.g., convexity [''', '''<h25>123''', '''<h44>] or uniformly bounded gradients [''', '''<h25>131''', '''<h44>]. There are also heuristic approaches that aim to tackle''', '''<h7>statistical heterogeneity, either by sharing local device data or some server-side proxy data [''', '''<h25>52  55  138''', '''<h7>]. However, these methods may be unrealistic: in addition to imposing burdens on network bandwidth,''', '''<h44>sending local data to the server [''', '''<h25>55''', '''<h44>] violates the key privacy assumption of federated learning, and sending''', '''<h7>globally-shared proxy data to all devices [''', '''<h25>52  138''', '''<h7>] requires effort 
to carefully generate or collect such''', '''<h25>auxiliary data.''', '''<h5>2.4 Privacy''', '''<h7>Privacy concerns often motivate the need to keep raw data on each device local in federated settings. However, sharing other information such as model updates as part of the training process can also leak''', '''<h15>sensitive user information [''', '''<h25> 17  39  78''', '''<h15>]. For instance, Carlini et al.''', '''<h25> [17]''', '''<h15> demonstrate that one can extract''', '''<h41>sensitive text patterns, e.g., a specific credit card number, from a recurrent neural network trained on users''', '''<h44>language data. Given increasing interest in privacy-preserving learning approaches, in 
Section 2.4.1, we first''', '''<h16>briefly revisit prior work on enhancing privacy in the general (distributed) machine learning setting. We''', '''<h35>then review recent 
privacy-preserving methods specifically designed for federated settings in Section 2.4.2.''', '''<h25>2.4.1 Privacy in Machine Learning''', '''<h24>Privacy-preserving learning has been extensively studied by the machine learning [e.g.,''', '''<h25> 76''', '''<h24>], systems [e.g.,''', '''<h25> 11''', '''<h12>], and theory [e.g.,''', '''<h25> 38  69''', '''<h12>] communities. Three main strategies, each of which we will briefly review,''', '''<h18>include differential privacy to communicate noisy data sketches, homomorphic encryption to operate on''', '''<h25>encrypted data, and secure function evaluation or multiparty computation.''', '''<h44>Among these various privacy approaches,  differential privacy  [''', '''<h25>32  33  34''', '''<h44>] is most widely used due to its strong''', '''<h7>information theoretic guarantees, algorithmic simplicity, and relatively small 
systems overhead. Simply''', '''<h34>put, a randomized mechanism is differentially private if the change of one input element will not result in''', '''<h7>too much difference in the output distribution; this means that one cannot draw any conclusions about''', '''<h44>whether or not a specific sample is used in the learning process. Such sample-level privacy can be achieved''', '''<h38>in many learning tasks [''', '''<h25> 7  22  53  85  86''', '''<h38>]. For gradient-based learning methods, a popular approach is to''', '''<h37>apply differential privacy by randomly perturbing the intermediate output at each iteration [e.g.,''', '''<h25> 2  7  126''', '''<h37>]. Before applying the perturbation, e.g., via Gaussian noise [ ], Laplacian noise [''', '''<h25>77''', '''<h37>], or Binomial noise [ ], it''', '''<h7>is common to clip the gradients in order to bound the influence of each example on the overall update.''', '''<h22>There exists an inherent trade-off between differential privacy and model accuracy, as adding more noise''', '''<h7>results in greater privacy, but may compromise accuracy significantly. Despite the fact that differential privacy is the de facto metric for privacy in machine learning, there are many other privacy definitions,''', '''<h43>such as''', '''<h25> k''', '''<h43>-anonymity [''', '''<h25>36''', '''<h43>],''', '''<h25> ''', '''<h43>-presence [''', '''<h25>81''', '''<h43>] and distance correlation [''', '''<h25>117''', '''<h43>], that may be applicable to different''', '''<h25>learning problems [118]. 10''', '''<s27>server devices''', '''<h47>(a) Federated learning with-''', '''<s9>out additional privacy protection''', '''<p>mechanisms.''', '''<s26>server devices''', '''<h47>(b) Global privacy, where a trusted''', '''<p>server 
is assumed.''', '''<s25>server devices''', '''<s15>(c) Local privacy, where the central server''', '''<p>might be malicious.''', '''<h44>Figure 6: An illustration of different privacy-enhancing mechanisms in one round of federated learning.''', '''<h6> M''', '''<h20>denotes a randomized mechanism used to privatize the data. With global privacy (b), the model updates''', '''<h21>are private to all third parties other than a single trusted party (the central server). With local privacy (c),''', '''<h25>the individual model 
updates are also private to the server.''', '''<h7>Beyond differential privacy, homomorphic encryption can be used to secure the learning process by computing on encrypted data, although it has currently been applied in limited settings, e.g., training linear models [''', '''<h25>82''', '''<h7>] or involving only a few entities [''', '''<h25>133''', '''<h7>]. When the sensitive datasets are distributed''', '''<h21>across different data owners, another natural option is to perform privacy-preserving learning via secure''', '''<h7>function evaluation (SFE) or secure multiparty computation (SMC). The resulting protocols can enable''', '''<h9>multiple parties to collaboratively compute an agreed-upon function without leaking input information''', '''<h28>from any party except for what can be inferred from the output [e.g.,''', '''<h25> 23  43  95''', '''<h28>]. Thus, while SMC cannot''', '''<h7>guarantee protection from information leakage, it can be combined with differential privacy to achieve''', '''<h16>stronger privacy guarantees. However, approaches along these lines may not be applicable to large-scale''', '''<h7>machine learning scenarios as they incur substantial additional communication and computation costs.''', '''<h44>Moreover, SMC protocols need to be carefully designed and implemented for each operation in the targeted''', '''<h36>learning algorithm [''', '''<h25>25  79''', '''<h36>]. We defer interested readers to [''', '''<h25>13  97''', '''<h36>] for a more comprehensive review of the''', '''<h25>approaches based on homomorphic encryption and SMC. 2.4.2 Privacy in Federated Learning''', '''<h7>The federated setting poses novel challenges to existing privacy-preserving algorithms. Beyond pro- viding rigorous privacy guarantees, it is necessary to develop methods that are computationally cheap, communication-efficient, and tolerant to dropped devicesall without overly compromising accuracy. Although there are a variety of privacy definitions in federated learning [''', '''<h25> 17  41  64  76  113''', '''<h7>], typically they can be classified into two categories:  global privacy  and  local privacy . As demonstrated in Figure 6,''', '''<h21>global privacy requires that the model updates generated at each round are private to all untrusted third''', '''<h44>parties other than the central server, while local privacy further requires that the updates are also private to''', '''<h25>the server.''', '''<h44>Current works that aim to improve the privacy of federated learning typically build upon previous classical''', '''<h7>cryptographic protocols such as SMC [''', '''<h25>10  42''', '''<h7>] and differential privacy [''', '''<h25> 8  41  76''', '''<h7>]. Bonawitz et al.''', '''<h25> [10]''', '''<h19>introduce an SMC protocol to protect individual model updates. The central server is not able to see any''', '''<h44>local updates, but can still observe the exact aggregated results at each 
round. SMC is a lossless method, and''', '''<h35>can retain the original accuracy with a very high privacy guarantee. However, the resulting method incurs''', '''<h33>significant extra communication cost. Other works [''', '''<h25>41  76''', '''<h33>] apply differential privacy to federated learning''', '''<img> p10-0.png''', '''<img> p10-1.png''', '''<img> p10-2.png''', '''<img> p10-3.png''', '''<img> p10-4.png''', '''<img> p10-5.png''', '''<h25>11''', '''<h7>and offer global differential privacy. These approaches have a number of hyperparameters that affect''', '''<h44>communication and accuracy that must be carefully chosen, though follow up work [''', '''<h25>113''', '''<h44>] proposes adaptive''', '''<h22>gradient clipping strategies to help alleviate this issue. In the case where stronger privacy guarantees are''', '''<h44>required, Bhowmick et al.''', '''<h25> [8]''', '''<h44> introduce a relaxed version of local privacy by limiting the power of potential''', '''<h29>adversaries. It affords stronger privacy guarantees than global privacy, and has 
better model performance''', '''<h7>than strict local privacy. Li et al.''', '''<h25> [64]''', '''<h7> propose locally differentially-private algorithms in the context of meta-learning, which can be applied to federated learning with personalization, while also providing provable learning guarantees in convex settings. In addition, differential privacy can be combined with''', '''<h25>model compression techniques to reduce communication and obtain privacy benefits simultaneously [3].''', '''<h3>Future Directions''', '''<h7>Federated learning is an active and ongoing area of research. Although recent work has begun to address the challenges discussed in Section 2, there are a 
number of critical open directions yet to be''', '''<h44>explored. In this section, we briefly outline a few promising research directions surrounding the previously''', '''<h7>discussed challenges (expensive communication, systems heterogeneity, statistical heterogeneity, and privacy concerns), and introduce additional challenges regarding issues such as productionizing and''', '''<h25>benchmarking in federated settings.  Extreme communication schemes.''', '''<h7> It remains to be seen how much communication is necessary in federated learning. Indeed, it is well-known that optimization methods for machine learning can tolerate a lack of precision; this error can in fact help with generalization [''', '''<h25>129''', '''<h7>]. While one- shot or divide-and-conquer communication schemes have been explored in traditional data center settings [''', '''<h25>73  137''', '''<h7>], the behavior of these methods is not well-understood in massive or statistical heterogeneous networks. Similarly, one-shot/few-shot heuristics [''', '''<h25>44  45  134''', '''<h7>] have recently been''', '''<h25>proposed for the federated setting, but have yet to be theoretically analyzed or evaluated at scale.  Communication reduction and the Pareto frontier.''', '''<h7> We discussed several ways to reduce commu- nication in federated training, such as local updating and model compression. In order to create a''', '''<h11>realistic system for federated learning, it is important to understand how these techniques  compose''', '''<h44>with one another, and to  systematically  analyze the trade-off between accuracy and communication for each approach. In particular, the most useful techniques will demonstrate improvements at the Pareto''', '''<h7>frontierachieving an accuracy greater than any other approach under the same communication''', '''<h41>budget, and ideally, across a 
wide range of communication/accuracy profiles. Similar comprehensive''', '''<h7>analyses have been performed for efficient neural network inference [e.g.,''', '''<h25> 9''', '''<h7>], and are necessary in''', '''<h25>order to compare communication-reduction techniques for federated learning in a meaningful way.  Novel models of asynchrony.''', '''<h7> As discussed in Section 2.2.1, two communication schemes most''', '''<h24>commonly studied in distributed optimization are bulk synchronous approaches and asynchronous''', '''<h41>approaches (where it is assumed that the delay is bounded). These schemes are more realistic in data''', '''<h7>center settingswhere worker nodes are typically  dedicated  to the workload, i.e., they are ready to''', '''<h24>pull their next job from the central node immediately after they push the results of their previous''', 
'''<h25>job. In contrast, in federated networks, each device is often  undedicated  to the task at hand and most''', '''<h22>devices are not active on any given iteration. Therefore, it is worth studying the effects of this more''', '''<h12>realistic  device-centric  communication schemein which each device can decide when to wake up''', '''<h25>and interact with the central server in an event-triggered manner. 12  Heterogeneity diagnostics.''', '''<h7> Recent works have aimed to quantify statistical heterogeneity through''', '''<h44>metrics such as local dissimilarity (as defined in the context of federated learning in [''', '''<h25>65''', '''<h44>] and used for''', '''<h7>other purposes in works such as [''', '''<h25>100  116  130''', '''<h7>]) and earth movers distance [''', '''<h25>138''', '''<h7>]. However, these''', '''<h44>metrics cannot be easily calculated over the federated network before training occurs. The importance''', '''<h7>of these metrics motivates the following open questions: (i) Do simple diagnostics exist to quickly''', '''<h37>determine the level of heterogeneity in federated networks  a priori ? (ii) Can analogous diagnostics be''', '''<h44>developed to quantify the amount of  
systems-related  heterogeneity? (iii) Can current or new definitions''', '''<h40>of heterogeneity be exploited to further improve the convergence of federated optimization methods?''', '''<h25> Granular privacy constraints.''', '''<h7> The definitions of privacy outlined in Section 2.4.2 cover privacy at a local or global level with respect to all devices in the network. However, in practice, it may be necessary to define privacy on a more granular level, as privacy constraints may differ across devices or even across data points on a single device. For instance, Li et al.''', '''<h25> [64]''', '''<h7> recently proposed sample-specific (as opposed to user-specific) privacy guarantees, thus providing a weaker form of''', '''<h39>privacy in exchange for more accurate models. Developing methods to handle mixed (device-specific''', '''<h25>or sample-specific) privacy restrictions is an interesting and ongoing direction of future work.  Beyond supervised learning.''', '''<h18> It is important to note that the methods discussed thus far have been''', '''<h7>developed with the task of  supervised learning  in mind, i.e., they assume that labels exist for all of the data in the federated network. In practice, much of the data generated in realistic federated''', '''<h34>networks may be unlabeled or weakly labeled. Furthermore, the problem at hand may not be to fit a''', '''<h28>model to data as presented in''', '''<h25> (1)''', '''<h28>, but instead to perform some exploratory data analysis, determine''', '''<h17>aggregate 
statistics, or run a more complex task such as reinforcement learning. Tackling problems''', '''<h44>beyond supervised learning in federated networks will likely require addressing similar challenges of''', '''<h25>scalability, heterogeneity, and privacy.  Productionizing federated learning.''', '''<h7> Beyond the major challenges discussed in this article, there are a number of practical concerns that arise when running federated learning in production. In particular, issues such as concept drift (when the underlying data-generation model changes over''', '''<h29>time); diurnal variations (when the devices exhibit different behavior at different times of the day or''', '''<h44>week) [''', '''<h25>35''', '''<h44>]; and cold start problems (when new devices enter the network) must be handled with care.''', '''<h42>We defer the readers to [''', '''<h25>11''', '''<h42>], which discusses some of the practical systems-related issues that exist in''', '''<h25>production federated learning systems.  Benchmarks.''', '''<h7> Finally, as federated learning is a nascent field, we are at a pivotal time to shape the developments made in this area and ensure that they are grounded in real-world settings,''', '''<h17>assumptions, and datasets. It is critical for the broader research communities to further build upon''', '''<h30>existing implementations and benchmarking tools, such as LEAF [''', '''<h25>16''', '''<h30>] and TensorFlow Federated [ ],''', '''<h17>to facilitate both the reproducibility of empirical results and the dissemination of new solutions for''', '''<h25>federated learning.''', '''<h3>Conclusion''', '''<h23>In this article, we have provided an overview of federated learning, a learning paradigm where statistical''', '''<h7>models are trained at the edge in distributed networks. We have discussed the unique properties and''', '''<h19>associated challenges of federated learning compared with traditional distributed data center computing''', '''<h22>and classical privacy-preserving learning. We provided an extensive survey on classical results as well as''', '''<h44>more recent work specifically focused on federated settings. Finally, we have outlined out a handful of open''', '''<h25>13''', '''<h34>problems 
worth future research effort. Providing solutions to these problems will require interdisciplinary''', '''<h25>effort from a broad set of research communities. Acknowledgement.''', '''<h44>We thank Jeffrey Li and Mikhail Khodak for helpful discussions and comments. This work was supported in part by DARPA FA875017C0141, the National Science Foundation grants IIS1705121''', '''<h29>and IIS1838017, an Okawa Grant, a Google Faculty Award, an Amazon Web Services Award, a JP Morgan''', '''<h32>A.I. Research Faculty Award, a Carnegie Bosch Institute Research Award and the CONIX Research Center,''', '''<h23>one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.''', '''<h11>Any opinions, findings, and conclusions or recommendations expressed in this material are those of the''', '''<h44>author(s) and do not necessarily reflect the views of DARPA, the National Science Foundation, or any other''', '''<h25>funding agency.''', '''<h3>References''', '''<p>[1]''', '''<s9> Tensorflow federated: Machine learning on decentralized data. URL''', '''<p> https://www.tensorflow.org/federated [2]''', '''<h47> M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with''', '''<p>differential privacy. In  Conference on Computer and Communications Security , 2016. [3]''', '''<h47> N. Agarwal, A. T. Suresh, F. X. X. Yu, S. Kumar, and B. McMahan. cpSGD: Communication-efficient and''', '''<p>differentially-private distributed sgd. In  Advances in Neural Information 
Processing Systems , 2018. [4]''', '''<h49> R. Agrawal and R. Srikant. Privacy-preserving data mining. In  International Conference on Management of Data''', '''<p>2000. [5]''', '''<s10> M. Ammad-ud din, E. Ivannikova, S. A. Khan, W. Oyomno, Q. Fu, K. E. Tan, and A. Flanagan. Federated collab-''', '''<h50>orative filtering for privacy-preserving personalized recommendation system.  arXiv preprint arXiv:1901.09888''', '''<p>2019. [6]''', '''<h47> D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz. A public domain dataset for human activity''', '''<s19>recognition using smartphones. In  European Symposium on Artificial Neural Networks, Computational Intelligence and''', '''<p>Machine Learning , 2013. [7]''', '''<h56> R. Bassily, A. Smith, and A. Thakurta. Private empirical risk minimization: Efficient algorithms and tight error''', '''<p>bounds. In  Foundations of Computer Science , 2014. [8]''', '''<h47> A. Bhowmick, J. Duchi, J. Freudiger, G. Kapoor, and R. Rogers. Protection against reconstruction and its''', '''<p>applications in private federated learning.  arXiv preprint arXiv:1812.00984 , 2018. [9]''', '''<s19> T. Bolukbasi, J. Wang, O. Dekel, and V. Saligrama. Adaptive neural networks for efficient inference. In  International''', '''<p>Conference on Machine Learning , 2017. [10]''', '''<s19> K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage, A. Segal, and K. Seth. Prac-''', '''<s6>tical secure aggregation for privacy-preserving machine learning. In  Conference on Computer and Communications''', '''<p>Security , 2017. [11]''', '''<s1> K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi,''', '''<h59>H. B. McMahan, T. V. Overveldt, D. Petrou, D. Ramage, and J. Roselander. Towards federated learning at scale:''', '''<p>system design. In  Conference on Systems and Machine Learning , 2019. [12]''', '''<s19> F. Bonomi, R. Milito, J. Zhu, and S. Addepalli. Fog computing and its role in the internet of things. In  SIGCOMM''', '''<p>Workshop on Mobile Cloud Computing , 2012.''', '''<h25>14''', '''<p>[13]''', '''<h52> R. Bost, R. A. Popa, S. Tu, and S. Goldwasser. Machine learning classification over encrypted data. In  Network''', '''<p>and Distributed System Security Symposium , 2015. [14]''', '''<h56> S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the''', '''<p>alternating direction method of multipliers.  Foundations and Trends''', '''<s24> R''', '''<p> in Machine Learning , 3:1122, 2011. [15]''', '''<s14> S. Caldas, J. Konecny, H. B. McMahan, and A. Talwalkar. Expanding the reach of federated learning by reducing''', '''<p>client resource requirements.  
arXiv preprint arXiv:1812.07210 , 2018. [16]''', '''<s18> S. Caldas, P. Wu, T. Li, J. Konecn y, H. B. McMahan, V. Smith, and A. Talwalkar. Leaf: A benchmark for federated''', '''<p>settings.  arXiv preprint arXiv:1812.01097 , 2018. [17]''', '''<s3> N. Carlini, C. Liu, J. Kos, . Erlingsson, and D. Song. The secret sharer: Measuring unintended neural network''', '''<p>memorization & extracting secrets.  arXiv preprint arXiv:1802.08232 , 2018. [18] R. Caruana. Multitask learning.  Machine Learning , 28:4175, 
1997. [19]''', '''<h47> M. Castro, B. Liskov, et al. Practical byzantine fault tolerance. In  Operating Systems Design and Implementation''', '''<p>1999. [20]''', '''<s7> Z. Charles and D. Papailiopoulos. Gradient coding using the stochastic block model. In  International Symposium''', '''<p>on Information Theory , 2018. [21]''', '''<h51> Z. B. Charles, D. S. Papailiopoulos, and J. Ellenberg. Approximate gradient coding via sparse random graphs.''', '''<p>arXiv preprint arXiv:1711.0677 , 2017. [22]''', '''<h54> K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization.  Journal of''', '''<p>Machine Learning Research , 12:10691109, 2011. [23]''', '''<h48> D. Chaum. The dining cryptographers problem: Unconditional sender and recipient untraceability.  Journal of''', '''<p>Cryptology , 1:6575, 1988. [24]''', '''<s19> F. Chen, Z. Dong, Z. Li, and X. He. Federated meta-learning for recommendation.  arXiv preprint arXiv:1802.07876''', '''<p>2018. [25]''', '''<h47> V. Chen, V. Pastro, and M. Raykova. Secure computation for machine learning with spdz.  arXiv preprint''', '''<p>arXiv:1901.00329 , 2019. [26]''', '''<s2> L. Corinzia and J. M. Buhmann. Variational federated multi-task learning.  arXiv preprint arXiv:1906.06268 , 2019.''', '''<p>[27]''', '''<h60> W. Dai, A. Kumar, J. Wei, Q. Ho, G. Gibson, and E. P. Xing. High-performance distributed ML at scale through''', '''<p>parameter server consistency models. In  AAAI Conference on Artificial Intelligence , 2015. [28]''', '''<h60> O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using mini-batches.''', '''<p>Journal of Machine Learning Research , 13:165202, 2012. [29] A. Deshpande, C. Guestrin, S. R. Madden, J. M. Hellerstein, and W. Hong. Model-based approximate querying in sensor networks.  The VLDB Journal , 14:417443, 2005. [30]''', '''<h47> J. Duchi, M. I. Jordan, and B. McMahan. Estimation, optimization, and parallelism when data is sparse. In''', '''<p>Advances in Neural Information Processing Systems , 2013. [31]''', '''<h47> J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Privacy aware learning. In  Advances in Neural Information''', '''<p>Processing Systems , 2012. 
[32] C. Dwork. A firm foundation for private data analysis.  Communications of the ACM , 54:8695, 2011. [33]''', '''<s1> C. Dwork and A. Roth. The algorithmic foundations of differential privacy.  Foundations and Trends in Theoretical''', '''<p>Computer Science , 9:211407, 2014.''', '''<h25>15''', '''<p>[34]''', '''<h47> C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In''', '''<p>Theory of Cryptography Conference , 2006. [35]''', '''<h47> H. Eichner, T. Koren, H. B. McMahan, N. Srebro, and K. Talwar. Semi-cyclic stochastic gradient descent. In''', '''<p>International Conference on Machine Learning , 2019. [36]''', '''<h60> K. El Emam and F. K. Dankar. Protecting privacy using k-anonymity.  Journal of the American Medical Informatics''', '''<p>Association , 15:627637, 2008. [37]''', '''<h47> T. Evgeniou and M. Pontil. Regularized multitask learning. In  Conference on Knowledge Discovery and Data''', '''<p>Mining , 2004. [38]''', '''<s19> V. Feldman, I. Mironov, K. Talwar, and A. Thakurta. Privacy amplification by iteration. In  Foundations of Computer''', '''<p>Science , 2018. [39]''', '''<h59> M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit confidence information and basic''', '''<p>countermeasures. In  Conference on Computer and Communications Security , 2015. [40]''', '''<h47> P. Garcia Lopez, A. Montresor, D. Epema, A. Datta, T. Higashino, A. Iamnitchi, M. Barcellos, P. Felber, and''', '''<s17>E. Riviere. Edge-centric computing: Vision and challenges.  SIGCOMM Computer Communication Review , 45:3742,''', '''<p>2015. [41]''', '''<h47> R. C. Geyer, T. Klein, and M. Nabi. Differentially private federated learning: A client level perspective.  arXiv''', '''<p>preprint arXiv:1712.07557 , 2017. [42]''', '''<h54> B. Ghazi, R. Pagh, and A. Velingker. Scalable and differentially private 
distributed aggregation in the shuffled''', '''<p>model.  arXiv preprint arXiv:1906.08320 , 2019. [43]''', '''<s15> S. Goryczka and L. Xiong. A comprehensive comparison of multiparty secure additions with differential privacy.''', '''<p>IEEE Transactions on Dependable and Secure Computing , 14:463477, 2015. [44]''', '''<s9> N. Guha and V. Smith. Model aggregation via good-enough model spaces.  arXiv preprint arXiv:1805.07782 , 2018.''', '''<p>[45] N. Guha, A. Talwalkar, and V. Smith. One-shot federated learning.  arXiv preprint arXiv:1902.11175 , 2019. [46]''', '''<h47> A. Hard, K. Rao, R. Mathews, F. Beaufays, S. Augenstein, H. Eichner, C. Kiddon, and D. Ramage. Federated''', 
'''<p>learning for mobile keyboard prediction.  arXiv preprint arXiv:1811.03604 , 2018. [47]''', '''<h47> L. He, A. Bian, and M. Jaggi. Cola: Decentralized linear learning. In  Advances in Neural Information Processing''', '''<p>Systems , 2018. [48]''', '''<h59> Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson, G. Ganger, and E. P. Xing. More effective''', '''<h47>distributed ML via a stale synchronous parallel parameter server. In  Advances in Neural Information Processing''', '''<p>Systems , 2013. [49]''', '''<s16> K. Hong, D. Lillethun, U. Ramachandran, B. Ottenwlder, and B. Koldehofe. Mobile fog: A programming model''', '''<p>for large-scale applications 
on the internet of things. In  SIGCOMM Workshop on Mobile Cloud Computing , 2013. [50]''', '''<s12> J. Huang, F. Qian, Y. Guo, Y. Zhou, Q. Xu, Z. M. Mao, S. Sen, and O. 
Spatscheck. An in-depth study of lte: effect''', '''<h56>of network protocol and application behavior on performance.  SIGCOMM Computer Communication Review , 43:''', '''<p>363374, 2013. [51]''', '''<h58> L. Huang and D. Liu. Patient clustering improves efficiency of federated machine learning to predict mortality''', '''<p>and hospital stay time using distributed electronic medical records.  arXiv preprint arXiv:1903.09296 , 2019. [52]''', '''<h47> L. Huang, Y. Yin, Z. Fu, S. Zhang, H. Deng, and D. Liu. Loadaboost: Loss-based adaboost federated machine''', '''<p>learning on medical data.  arXiv preprint arXiv:1811.12629 , 2018. [53]''', '''<h50> R. Iyengar, J. P. Near, D. Song, 
O. Thakkar, A. Thakurta, and L. Wang. Towards practical differentially private''', '''<p>convex optimization. In  Conference on Computer and Communications Security , 2019.''', '''<h25>16''', '''<p>[54]''', '''<h50> M. Jaggi, V. Smith, M. Takc, J. Terhorst, S. Krishnan, T. Hofmann, and M. I. Jordan. Communication-efficient''', '''<p>distributed dual coordinate ascent. In  Advances in Neural Information Processing Systems , 2014. [55]''', '''<s12> E. Jeong, S. Oh, H. Kim, J. Park, M. Bennis, and S.-L. Kim. Communication-efficient on-device machine learning:''', '''<p>Federated distillation and augmentation under non-iid private data.  arXiv preprint arXiv:1811.11479 , 2018. [56]''', '''<h47> P. Jiang and G. Agrawal. A linear speedup analysis of distributed deep learning with sparse and quantized''', '''<p>communication. In  Advances in Neural Information Processing Systems , 2018. [57] J. Kang, Z. Xiong, D. Niyato, H. Yu, Y.-C. Liang, and D. I. Kim. Incentive design for efficient federated learning in mobile networks: A contract theory approach.  arXiv preprint arXiv:1905.07479 , 2019. [58]''', '''<h47> M. Khodak, M.-F. Balcan, and A. Talwalkar. Adaptive gradient-based meta-learning methods.  arXiv preprint''', '''<p>arXiv:1906.02717 , 2019. [59]''', '''<s7> J. Konecn y, H. B. McMahan, F. X. Yu, P. Richtrik, A. T. Suresh, and D. Bacon. Federated learning: strategies for''', '''<p>improving communication efficiency.  arXiv preprint arXiv:1610.05492 , 2016. [60]''', '''<h47> T. Kuflik, J. Kay, and B. Kummerfeld. Challenges and solutions of ubiquitous user modeling. In  Ubiquitous''', '''<p>Display Environments . 2012. [61]''', '''<h60> A. Lalitha, X. Wang, O. Kilinc, Y. Lu, T. Javidi, and F. Koushanfar. Decentralized bayesian learning over graphs.''', '''<p>arXiv preprint arXiv:1905.10466 , 2019. [62]''', '''<h58> C.-P. Lee and D. Roth. Distributed box-constrained quadratic optimization for dual linear svm. In  International''', '''<p>Conference on Machine Learning , 2015. [63]''', '''<h47> K. Lee, M. Lam, R. Pedarsani, D. Papailiopoulos, and K. Ramchandran. Speeding up distributed machine''', '''<p>learning using codes.  IEEE Transactions on Information Theory , 64:15141529, 2017. [64]''', '''<h47> J. Li, M. Khodak, S. Caldas, and A. Talwalkar. Differentially-private gradient-based meta-learning.  Technical''', '''<p>Report , 2019. [65]''', '''<h60> T. Li, A. K. Sahu, M. Sanjabi, M. Zaheer, A. Talwalkar, and V. Smith. Federated optimization for heterogeneous''', '''<p>networks.  arXiv preprint arXiv:1812.06127 , 2018. [66]''', '''<h47> T. Li, M. Sanjabi, and V. Smith. Fair resource allocation in federated learning.  arXiv preprint arXiv:1905.10497''', '''<p>2019. [67]''', '''<h47> X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu. Can decentralized algorithms outperform''', '''<s11>centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In  Advances in Neural''', '''<p>Information Processing Systems , 2017. [68]''', '''<s19> T. Lin, S. U. Stich, and M. Jaggi. Dont use large mini-batches, use local sgd.  arXiv preprint arXiv:1808.07217 , 2018.''', '''<p>[69] Y. Lindell and B. Pinkas. Privacy preserving data mining. In  Advances in Cryptology , 2000. [70]''', '''<s19> L. Liu, J. Zhang, S. Song, and K. B. Letaief. Edge-assisted hierarchical federated learning with non-iid data.  arXiv''', '''<p>preprint arXiv:1905.06641 , 2019. [71]''', '''<s11> Y. Liu, J. K. Muppala, M. Veeraraghavan, D. Lin, and M. Hamdi.  Data center networks: Topologies, architectures and''', '''<p>fault-tolerance characteristics . Springer Science & Business Media, 2013. [72]''', '''<h47> C. Ma, V. Smith, M. Jaggi, M. I. Jordan, P. Richtrik, and M. Takc. Adding vs. averaging in distributed''', '''<p>primal-dual optimization. In  International Conference on Machine Learning , 2015. [73]''', '''<h48> L. W. Mackey, M. I. Jordan, and A. Talwalkar. Divide-and-conquer 
matrix factorization. In  Advances in Neural''', '''<p>Information Processing Systems , 2011. [74]''', '''<s3> S. R. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong. 
Tinydb: an acquisitional query processing system''', '''<p>for sensor networks.  Transactions on Database Systems , 30:122173, 2005.''', '''<h25>17''', '''<p>[75]''', '''<h47> H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of''', '''<p>deep networks from decentralized data. In  Conference on Artificial Intelligence and Statistics , 2017. [76]''', '''<s15> H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private recurrent language models.''', '''<p>In  International Conference on Learning Representations , 2018. [77]''', '''<h47> L. Melis, G. Danezis, and E. D. Cristofaro. Efficient private statistics with 
succinct sketches. In  Network and''', '''<p>Distributed System Security Symposium , 2016. [78]''', '''<h47> L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov. Exploiting unintended feature leakage in collaborative''', '''<p>learning. In  IEEE Symposium on Security & Privacy , 2019. [79]''', '''<s2> P. Mohassel and P. Rindal. Aby 3: a mixed protocol framework for machine learning. In  Conference on Computer''', '''<p>and Communications Security , 2018. [80]''', '''<s19> M. Mohri, G. Sivek, and A. T. Suresh. Agnostic federated learning. In  International Conference on Machine Learning''', '''<p>2019. [81]''', '''<s9> M. E. Nergiz and C. Clifton.''', '''<p> ''', '''<s9>-presence without complete world knowledge.  IEEE Transactions on Knowledge and''', '''<p>Data Engineering , 22:868883, 2010. [82]''', '''<h57> V. Nikolaenko, U. Weinsberg, S. Ioannidis, M. Joye, D. Boneh, and N. Taft. Privacy-preserving ridge regression''', '''<p>on hundreds of millions of records. In  Symposium on Security and Privacy , 2013. [83] T. Nishio 
and R. Yonetani. Client selection for federated learning with heterogeneous resources in mobile edge. In  International Conference on Communications , 2019. [84]''', '''<h56> A. Pantelopoulos and N. G. Bourbakis. A survey on wearable sensor-based systems for health monitoring and''', '''<p>prognosis.  IEEE Transactions on Systems, Man, 
and Cybernetics , 40:112, 2010. [85]''', '''<h47> N. Papernot, M. Abadi, U. Erlingsson, I. Goodfellow, and K. Talwar. Semi-supervised knowledge transfer for''', '''<p>deep learning from private training data. In  International Conference on Learning Representations , 2017. [86]''', '''<s1> N. Papernot, S. Song, I. Mironov, A. Raghunathan, K. Talwar, and . Erlingsson. Scalable private learning with''', '''<p>pate. In  International Conference on Learning Representations , 2018. [87]''', '''<h47> A. Qiao, B. Aragam, B. Zhang, and E. Xing. Fault tolerance in iterative-convergent machine learning. In''', '''<p>International Conference on Machine Learning , 2019. [88]''', '''<h47> Z. 
Qu, P. Richtrik, and T. Zhang. Quartz: Randomized dual coordinate ascent with arbitrary sampling. In''', '''<p>Advances in Neural Information Processing Systems , 2015. 
[89]''', '''<h47> S. Ramaswamy, R. Mathews, K. Rao, and F. Beaufays. Federated learning for emoji prediction in a mobile''', '''<p>keyboard.  arXiv preprint arXiv:1906.04329 , 2019. [90]''', '''<s19> M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net: Imagenet classification using binary convolutional''', '''<p>neural networks. In  European Conference on Computer Vision , 2016. [91] A. Ratner et al. SysML: The new frontier of machine learning systems.  arXiv preprint arXiv:1904.03257 , 2019. [92]''', '''<s19> B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent.''', '''<p>In  Advances in Neural Information Processing Systems , 2011. [93]''', '''<s18> S. J. Reddi, J. Konecn y, P. Richtrik, B. Pczs, and A. Smola. Aide: Fast and communication efficient distributed''', '''<p>optimization.  arXiv preprint arXiv:1608.06879 , 2016. [94]''', '''<s3> A. Reisizadeh, S. Prakash, R. Pedarsani, and A. S. Avestimehr. Coded computation over heterogeneous clusters.''', '''<p>IEEE Transactions on Information Theory , 65:42274242, 2019.''', '''<h25>18''', '''<p>[95]''', '''<h47> M. S. Riazi, C. Weinert, O. Tkachenko, E. M. Songhori, T. Schneider, and F. Koushanfar. Chameleon: A hybrid secure computation framework for machine learning applications. In  Asia Conference on Computer and''', '''<p>Communications Security , 2018. [96]''', '''<s4> P. Richtrik and M. Takc. Distributed coordinate descent method for learning with big data.  Journal of Machine''', '''<p>Learning Research , 17:26572681, 2016. [97]''', '''<h51> B. D. Rouhani, M. S. Riazi, and F. Koushanfar. Deepsecure: Scalable provably-secure deep learning. In  Design''', '''<p>Automation Conference , 2018. [98]''', '''<h47> S. Samarakoon, M. Bennis, W. Saad, and M. Debbah. Federated learning for ultra-reliable low-latency v2v''', '''<p>communications. In  Global Communications Conference , 2018. [99]''', '''<h53> F. Sattler, S. Wiedemann, K.-R. Mller, and W. Samek. Robust and communication-efficient federated learning''', '''<p>from non-iid data.  arXiv preprint arXiv:1903.02891 , 2019. [100]''', '''<h53> M. Schmidt and N. L. Roux. Fast convergence of stochastic gradient descent under a strong growth condition.''', '''<p>arXiv preprint arXiv:1308.6370 , 2013. [101]''', '''<s5> F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and its application to data-parallel''', '''<p>distributed training of speech dnns. In  International Speech Communication Association , 2014. [102]''', '''<s12> S. Shalev-Shwartz and T. Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In  Advances in Neural''', '''<p>Information Processing Systems , 2013. [103]''', '''<s19> O. Shamir and N. Srebro. Distributed stochastic optimization and learning. In  Allerton Conference on Communication,''', '''<p>Control, and Computing , 2014. [104]''', '''<h48> O. Shamir, N. Srebro, and T. Zhang. Communication-efficient distributed optimization using an approximate''', '''<p>newton-type method. In  International Conference on Machine Learning , 2014. [105]''', '''<s16> S. Silva, B. Gutman, E. Romero, P. M. Thompson, A. Altmann, and M. Lorenzi. Federated learning in distributed''', '''<p>medical databases: Meta-analysis of large-scale subcortical brain data.  arXiv preprint arXiv:1810.08553 , 2018. [106]''', '''<h47> V. Smith, C.-K. Chiang, M. Sanjabi, and A. Talwalkar. Federated multi-task learning. In  Advances in Neural''', '''<p>Information Processing Systems , 2017. [107]''', '''<s6> V. Smith, S. Forte, C. Ma, M. Takac, M. I. Jordan, and M. Jaggi. Cocoa: a general framework for communication-''', '''<p>efficient distributed optimization.  Journal of Machine Learning Research , 18:147, 2018. [108]''', '''<s19> S. U. Stich. Local sgd converges fast and communicates little. In  International Conference on Learning Representations''', '''<p>2019. [109]''', '''<h56> R. Tandon, Q. Lei, A. G. Dimakis, and N. Karampatziakis. Gradient coding: Avoiding stragglers in distributed''', '''<p>learning. In  International Conference on Machine Learning , 2017. [110] A. S. Tanenbaum and M. Van Steen.  Distributed systems: principles and paradigms . Prentice-Hall, 2007. [111]''', '''<h47> H. Tang, S. Gan, C. Zhang, T. 
Zhang, and J. Liu. Communication compression for decentralized training. In''', '''<p>Advances in Neural Information Processing Systems , 2018. [112]''', '''<h47> H. Tang, C. Yu, C. Renggli, S. Kassing, A. Singla, D. Alistarh, J. Liu, and C. Zhang. Distributed learning over''', '''<p>unreliable networks. In  International Conference on Machine Learning , 2019. [113]''', '''<s19> O. Thakkar, G. Andrew, and H. B. McMahan. Differentially private learning with adaptive clipping.  arXiv preprint''', '''<p>arXiv:1905.03871 , 2019. [114] S. Thrun and L. Pratt.  Learning to learn . Springer Science & Business Media, 2012. [115] C. Van Berkel. Multi-core for mobile phones. In  Conference on Design, Automation and Test in Europe , 2009.''', '''<h25>19''', '''<p>[116]''', '''<s11> S. Vaswani, F. Bach, and M. Schmidt. Fast and faster convergence of sgd for 
over-parameterized models (and an''', '''<p>accelerated perceptron). In  Conference on Artificial Intelligence and Statistics , 2019. [117]''', '''<s4> P. Vepakomma, O. Gupta, A. Dubey, and R. Raskar. Reducing leakage in distributed deep learning for sensitive''', '''<p>health data.  arXiv preprint arXiv:1812.00564 , 2019. [118] I. Wagner and D. Eckhoff. Technical privacy metrics: a systematic survey.  ACM Computing Surveys , 51:57, 2018. [119]''', '''<h47> H. Wang, S. Sievert, S. Liu, Z. Charles, D. Papailiopoulos, and S. Wright. Atomo: Communication-efficient''', '''<p>learning via atomic sparsification. In  Advances in Neural Information Processing Systems , 2018. [120]''', '''<h47> J. Wang and G. Joshi. Cooperative sgd: A unified framework for the design and analysis of communication-''', '''<p>efficient sgd algorithms.  arXiv preprint arXiv:1808.07576 , 2018. [121]''', '''<h47> J. Wang and G. Joshi. Adaptive communication strategies to achieve the best error-runtime trade-off in local-''', '''<p>update sgd. In  Conference on Systems and Machine Learning , 2019. [122]''', '''<s19> S. Wang, F. Roosta-Khorasani, P. Xu, and M. W. Mahoney. Giant: Globally improved approximate 
newton method''', '''<p>for distributed optimization. In  Advances in Neural Information Processing Systems , 2018. [123]''', '''<h47> S. Wang, T. Tuor, T. Salonidis, K. K. 
Leung, C. Makaya, T. He, and K. Chan. Adaptive federated learning in''', '''<p>resource constrained edge computing systems.  Journal on Selected Areas in Communications 
, 37:12051221, 2019. [124] WeBank AI Group. Federated learning white paper v1.0. 2018. [125]''', '''<s19> B. Woodworth, J. Wang, A. Smith, B. McMahan, and N. Srebro. Graph oracle models, lower bounds, and gaps for''', '''<p>parallel stochastic optimization. In  Advances in Neural Information Processing Systems , 2018. [126]''', '''<s18> X. 
Wu, F. Li, A. Kumar, K. Chaudhuri, S. Jha, and J. Naughton. Bolt-on differential privacy for scalable stochastic''', '''<p>gradient descent-based analytics. In  International Conference on Management of Data , 2017. [127]''', '''<s11> Q. Yang, Y. Liu, T. Chen, and Y. Tong. Federated machine learning: Concept and applications.  ACM Transactions''', '''<p>on Intelligent Systems and Technology , 10:12, 2019. [128]''', '''<s3> T. Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent. In  Advances in''', '''<p>Neural Information Processing Systems , 2013. [129]''', '''<s19> Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning.  Constructive Approximation''', '''<p>26:289315, 2007. [130]''', '''<h47> D. Yin, A. Pananjady, M. Lam, D. Papailiopoulos, K. Ramchandran, and P. Bartlett. Gradient diversity: a key''', '''<h61>ingredient for scalable distributed learning. In  Conference on Artificial Intelligence and Statistics , pages 19982007,''', '''<p>2018. [131]''', '''<s4> H. Yu, S. Yang, and S. Zhu. Parallel restarted sgd for non-convex optimization with faster convergence and less''', '''<p>communication. In  AAAI Conference on Artificial Intelligence , 2018. [132]''', '''<h47> H. Yu, R. Jin, and S. Yang. On the linear speedup analysis of communication efficient momentum sgd for''', '''<p>distributed non-convex optimization. In  International Conference on Machine Learning , 2019. [133]''', '''<h47> J. Yuan and S. Yu. Privacy preserving back-propagation neural network learning made practical with cloud''', '''<p>computing.  IEEE Transactions on Parallel and Distributed Systems , 25:212221, 2013. [134]''', '''<h50> M. Yurochkin, M. Agarwal, S. Ghosh, K. Greenewald, T. N. Hoang, and Y. Khazaeni. Bayesian nonparametric''', '''<p>federated learning of neural networks. In  International Conference 
on Machine Learning , 2019. [135]''', '''<h58> H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang. ZipML: Training linear models with end-to-end low''', '''<p>precision, and a little bit of deep learning. In  International Conference on Machine Learning , 2017. [136]''', '''<h48> S. Zhang, A. E. Choromanska, and Y. LeCun. Deep learning with elastic averaging sgd. In  Advances in Neural''', '''<p>Information Processing Systems , 2015.''', '''<h25>20''', '''<p>[137]''', '''<h50> Y. Zhang, J. Duchi, and M. Wainwright. Divide and conquer kernel ridge regression: A distributed algorithm''', '''<p>with minimax optimal rates.  Journal of Machine Learning Research , 16:32993340, 2015. [138]''', '''<h55> Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra. Federated learning with non-iid data.  arXiv preprint''', '''<p>arXiv:1806.00582 , 2018. [139]''', '''<h47> Y. Zhao, J. Zhao, L. Jiang, R. Tan, and D. Niyato. Mobile edge computing, blockchain and reputation-based crowdsourcing iot federated learning: A secure, decentralized and privacy-preserving system.  arXiv preprint''', '''<p>arXiv:1906.10893 , 2019. [140]''', '''<s14> F. Zhou and G. Cong. On the convergence properties of a''', '''<p> k''', '''<s14>-step averaging stochastic gradient descent algorithm''', '''<p>for nonconvex optimization. In  International Joint Conference on Artificial Intelligence , 2018. [141]''', '''<s1> M. Zinkevich, M. Weimer, L. Li, and A. J. Smola. Parallelized stochastic gradient descent. In  Advances in Neural''', '''<p>Information Processing Systems , 2010.''', '''<h25>21''']

import trial_ppt as tp
tp.pptgen(tp.preprocessing('1908.07873.pdf','./trial'),'crap.pdf',1,'./trial')